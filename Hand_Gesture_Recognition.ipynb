{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hand Gesture Recognition.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMJqv8qXF1yPjGhZLY0Oryd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samsontran/Hand-Gesture-Recognition/blob/main/Hand_Gesture_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### American Sign Language\n",
        "\n",
        "American Sign Language (ASL) is a complete, complex language that employs signs made by moving the\n",
        "hands combined with facial expressions and postures of the body. It is the primary language of many\n",
        "North Americans who are deaf and is one of several communication options used by people who are deaf or\n",
        "hard-of-hearing.\n",
        "\n",
        "The hand gestures representing English alphabet are shown below. This lab focuses on classifying a subset\n",
        "of these hand gesture images using convolutional neural networks. Specifically, given an image of a hand\n",
        "showing one of the letters A-I, we want to detect which letter is being represented.\n",
        "\n",
        "![alt text](https://www.disabled-world.com/pics/1/asl-alphabet.jpg)\n"
      ],
      "metadata": {
        "id": "tFZP7q1nx0gN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Part 1 of this project, I write build a Convolutional Neural Network from scratch to train a model to recognize American Sign Language alphabets from A to I. \n",
        "\n",
        "In Part 2, I apply transfer learning using the neural network component of  AlexNet network to compute the convolutional features and get pre-trained weights. As you will see, the test accuracy goes up to 93%!\n",
        "\n",
        "In Part 3, I take my own American Sign Language hand gesture photos and test it out in my model (spoiler: my model accurately predicts all of them!)\n",
        "\n",
        "Dataset source: I use a labelled training set that was made available from my graduate level course, MIE1517 Deep Learning. "
      ],
      "metadata": {
        "id": "RXmXDc-Jxw02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import os, shutil\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "vF8tv27tyY_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDEOSj0Uyd_U",
        "outputId": "bfcc8134-71a0-458a-a005-880e393e1d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data pre-processing and loading"
      ],
      "metadata": {
        "id": "sYt2XLfFyqRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main = '/content/gdrive/MyDrive/Gesture_Dataset/'\n",
        "\n",
        "#create main directory folders for test, val, and train\n",
        "testMain = \"/content/gdrive/MyDrive/Lab_2B/test\"\n",
        "valMain = \"/content/gdrive/MyDrive/Lab_2B/val\"\n",
        "trainMain = \"/content/gdrive/MyDrive/Lab_2B/train\"\n",
        "if not os.path.exists(testMain):\n",
        "    os.makedirs(testMain)\n",
        "if not os.path.exists(valMain):\n",
        "    os.makedirs(valMain)\n",
        "if not os.path.exists(trainMain):\n",
        "    os.makedirs(trainMain)"
      ],
      "metadata": {
        "id": "PN3w-TyVygPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for subclass in os.listdir(main):\n",
        "  subfolder = os.path.join(main, str(subclass))\n",
        "  files_in_subfolder = sorted(os.listdir(subfolder))\n",
        "\n",
        "  #for each class, create a subfolder in the 'train' main directory folder\n",
        "  #split the data into 64% training, 16% validation, 20% testing\n",
        "  \n",
        "  trainSub = os.path.join(trainMain, subclass)\n",
        "  if not os.path.exists(trainSub):\n",
        "    os.makedirs(trainSub)\n",
        "\n",
        "  for file in files_in_subfolder[:math.floor(len(files_in_subfolder)*0.8*0.8)]:\n",
        "    shutil.copy(subfolder + '/' + file, trainSub + '/' + file)\n",
        "\n",
        "  valSub = os.path.join(valMain, subclass)\n",
        "  if not os.path.exists(valSub):\n",
        "    os.makedirs(valSub)\n",
        "\n",
        "  for file in files_in_subfolder[math.floor(len(files_in_subfolder)*0.8*0.8):math.floor(len(files_in_subfolder)*0.8)]:\n",
        "    shutil.copy(subfolder + '/' + file, valSub + '/' + file)\n",
        "\n",
        "  testSub = os.path.join(testMain, subclass)\n",
        "  if not os.path.exists(testSub):\n",
        "    os.makedirs(testSub)\n",
        "\n",
        "  for file in files_in_subfolder[math.floor(len(files_in_subfolder)*0.8):]:\n",
        "    shutil.copy(subfolder + '/' + file, testSub + '/' + file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "v_p0aZvUypCT",
        "outputId": "14024369-dea3-4fa5-fd0b-019130f65b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-06572da97d80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles_in_subfolder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_in_subfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainSub\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mvalSub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalMain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/content/gdrive/MyDrive/Gesture_Dataset/test/A'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define transform \n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize([224, 224]), #ensure all photos will be 224x224\n",
        "    transforms.CenterCrop(224), #assume photos have hand centered\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]) #normalize with mean and std values of 0.5\n",
        "\n",
        "# construct and transform the training, validation, and testing datasets separately \n",
        "train_dataset = torchvision.datasets.ImageFolder('/content/gdrive/MyDrive/Lab_2B/train',\n",
        "                                           transform=transform)\n",
        "val_dataset = torchvision.datasets.ImageFolder('/content/gdrive/MyDrive/Lab_2B/val',\n",
        "                                           transform=transform)\n",
        "test_dataset = torchvision.datasets.ImageFolder('/content/gdrive/MyDrive/Lab_2B/test',\n",
        "                                           transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "QUUAdsdvyxDZ",
        "outputId": "479c2fbc-de5b-4dc4-de1c-6964fc3c1040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-cf6766099251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# construct and transform the training, validation, and testing datasets separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m train_dataset = torchvision.datasets.ImageFolder('/content/gdrive/MyDrive/Lab_2B/train',\n\u001b[0;32m---> 10\u001b[0;31m                                            transform=transform)\n\u001b[0m\u001b[1;32m     11\u001b[0m val_dataset = torchvision.datasets.ImageFolder('/content/gdrive/MyDrive/Lab_2B/val',\n\u001b[1;32m     12\u001b[0m                                            transform=transform)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    311\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    144\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m    145\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;34m\"The class_to_idx parameter cannot be None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mextensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"Supported extensions are: {', '.join(extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Found no valid file for the classes B, C, D, E, H. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize the dataset\n",
        "train_loader = get_data_loader(target_classes=[\"car\", \"truck\"],  batch_size=1) # One image per batch\n",
        "\n",
        "k = 0\n",
        "for images, labels in train_loader:\n",
        "    # since batch_size = 1, there is only 1 image in `images`\n",
        "    image = images[0]\n",
        "    # place the colour channel at the end, instead of at the beginning\n",
        "    img = np.transpose(image, [1,2,0])\n",
        "    # normalize pixel intensity values to [0, 1]\n",
        "    img = img / 2 + 0.5\n",
        "    plt.subplot(3, 5, k+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "\n",
        "    k += 1\n",
        "    if k > 14:\n",
        "        break"
      ],
      "metadata": {
        "id": "79kxzGXbzFXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part 1: Building/training a convolutional neural network model"
      ],
      "metadata": {
        "id": "toue7Heh0xXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I'll define a model as follows:\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 3, 5)\n",
        "    self.pool = nn.MaxPool2d(5, 5)\n",
        "    self.conv2 = nn.Conv2d(3, 3, 5)\n",
        "    self.fc1 = nn.Linear(192, 32)\n",
        "    self.fc2 = nn.Linear(32,9)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu((self.conv1(x))))\n",
        "    x = self.pool(F.relu((self.conv2(x))))\n",
        "    x = x.view(-1, 192)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    x = x.squeeze(1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "XvhKOSEN0weE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now for the actual training...\n",
        "\n",
        "def train(model, training_data, val_data, batch_size = 128, learning_rate = 0.01, num_epochs = 20):\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(dataset=training_data, shuffle = True, batch_size=batch_size)\n",
        "\n",
        "  #define criterion & optimizer\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "  \n",
        "  train_losses_list=[]\n",
        "  train_accu_list=[]\n",
        "\n",
        "  eval_losses_list=[]\n",
        "  eval_accu_list=[]\n",
        "\n",
        "  for epoch in range(1,num_epochs+1): \n",
        "      print('\\nEpoch : %d'%epoch)\n",
        "\n",
        "      running_loss=0\n",
        "      incorrect=0\n",
        "      total=0\n",
        "\n",
        "      eval_running_loss=0\n",
        "      eval_incorrect=0\n",
        "      eval_total=0\n",
        "\n",
        "      for data in tqdm(train_loader):\n",
        "        \n",
        "            inputs,labels=data\n",
        "\n",
        "            #############################################\n",
        "            #To Enable GPU Usage\n",
        "            if use_cuda and torch.cuda.is_available():\n",
        "              inputs = inputs.cuda()\n",
        "              labels = labels.cuda()\n",
        "            #############################################\n",
        "            \n",
        "            outputs=model(inputs)\n",
        "            loss=criterion(outputs,labels)\n",
        "            loss.backward()  \n",
        "            optimizer.step()              \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            incorrect += predicted.ne(labels).sum().item()\n",
        "\n",
        "          \n",
        "      train_loss=running_loss/len(train_loader)\n",
        "      train_accu=100.*incorrect/total\n",
        "      \n",
        "      train_accu_list.append(train_accu)\n",
        "      train_losses_list.append(train_loss)\n",
        "      print('Train Loss: %.3f | Error: %.3f'%(train_loss,train_accu))\n",
        "\n",
        "      testloader = torch.utils.data.DataLoader(dataset=val_data, shuffle = True, batch_size=batch_size)\n",
        "\n",
        "      for data in tqdm(testloader):\n",
        "          inputs,labels=data\n",
        "\n",
        "          #############################################\n",
        "          #To Enable GPU Usage\n",
        "          if use_cuda and torch.cuda.is_available():\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "          #############################################\n",
        "\n",
        "          outputs=model(inputs)\n",
        "\n",
        "          loss= criterion(outputs,labels)\n",
        "          eval_running_loss+=loss.item()\n",
        "\n",
        "          _, predicted = outputs.max(1)\n",
        "          eval_total += labels.size(0)\n",
        "          eval_incorrect += predicted.ne(labels).sum().item()\n",
        "\n",
        "      test_loss=eval_running_loss/len(testloader)\n",
        "      eval_accu=100.*eval_incorrect/eval_total\n",
        "\n",
        "      eval_losses_list.append(test_loss)\n",
        "      eval_accu_list.append(eval_accu)\n",
        "\n",
        "      print('Val Loss: %.3f | Error: %.3f'%(test_loss,eval_accu))\n",
        "\n",
        "  return train_losses_list, train_accu_list, eval_losses_list, eval_accu_list"
      ],
      "metadata": {
        "id": "eRldP8Er1Czd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_curve(train_loss, train_err, val_loss, val_err):\n",
        "\n",
        "    plt.plot(train_err)\n",
        "    plt.plot(val_err)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('Error')\n",
        "    plt.legend(['Train','Valid'])\n",
        "    plt.title('Train vs Valid Error')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(train_loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('losses')\n",
        "    plt.legend(['Train','Valid'])\n",
        "    plt.title('Train vs Validation Losses')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RRjr2SN81PXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = True\n",
        "\n",
        "model = Classifier()\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "\n",
        "train_loss, train_err, val_loss, val_err = train(model, train_dataset, val_dataset, learning_rate = 0.005, num_epochs=15)\n",
        "\n",
        "plot_training_curve(train_loss, train_err, val_loss, val_err)\n",
        "\n",
        "print(\"Lowest Validation Error: {}, at Epoch {}\".format(min(val_err), np.argmin(val_err) + 1))\n",
        "print(\"Lowest Validation Loss: {}, at Epoch {}\".format(min(val_loss), np.argmin(val_err) + 1))"
      ],
      "metadata": {
        "id": "i-7MkXuW1U7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reporting test accuracy on test set..."
      ],
      "metadata": {
        "id": "yEUJjUdq2QqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "eval_total = 0\n",
        "eval_incorrect = 0\n",
        "\n",
        "eval_accu_list = []\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(dataset=test_dataset, shuffle = True, batch_size=128)\n",
        "\n",
        "for data in tqdm(testloader):\n",
        "    inputs,labels=data\n",
        "\n",
        "    #############################################\n",
        "    #To Enable GPU Usage\n",
        "    if use_cuda and torch.cuda.is_available():\n",
        "      inputs = inputs.cuda()\n",
        "      labels = labels.cuda()\n",
        "    #############################################\n",
        "\n",
        "    outputs=model3(inputs)\n",
        "\n",
        "    loss= criterion(outputs,labels)\n",
        "\n",
        "    _, predicted = outputs.max(1)\n",
        "    eval_total += labels.size(0)\n",
        "    eval_incorrect += predicted.ne(labels).sum().item()\n",
        "\n",
        "eval_accu=100.*eval_incorrect/eval_total\n",
        "\n",
        "print(\"Test accuracy is: \", 100 - eval_accu)"
      ],
      "metadata": {
        "id": "gUDK9VDJ2V3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part 2: Transfer Learning with AlexNet"
      ],
      "metadata": {
        "id": "ugByJ1l74Foc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models\n",
        "alexNet = torchvision.models.alexnet(pretrained=True)"
      ],
      "metadata": {
        "id": "H7KyhJGv4JxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Computing the AlexNet features for each of the training, validation, and test data\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128)\n",
        "\n",
        "train_features = torch.FloatTensor([])\n",
        "train_labels = torch.FloatTensor([])\n",
        "for images, labels in iter(train_loader):\n",
        "  train_features = torch.cat((train_features, alexNet.features(images)), 0)\n",
        "  train_labels = torch.cat((train_labels, labels), 0)\n",
        "\n",
        "val_features = torch.FloatTensor([])\n",
        "val_labels = torch.FloatTensor([])\n",
        "for images, labels in iter(val_loader):\n",
        "  val_features = torch.cat((val_features, alexNet.features(images)), 0)\n",
        "  val_labels = torch.cat((val_labels, labels), 0)\n",
        "\n",
        "test_features = torch.FloatTensor([])\n",
        "test_labels = torch.FloatTensor([])\n",
        "for images, labels in iter(test_loader):\n",
        "  test_features = torch.cat((test_features, alexNet.features(images)), 0)\n",
        "  test_labels = torch.cat((test_labels, labels), 0)"
      ],
      "metadata": {
        "id": "atfXtq5l4a0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the features to my drive\n",
        "\n",
        "path = F\"/content/gdrive/My Drive/lab_2_features/\" \n",
        "\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "torch.save(train_features, path + 'train_features.pt')\n",
        "torch.save(val_features, path + 'val_features.pt')\n",
        "torch.save(test_features, path + 'test_features.pt')\n",
        "\n",
        "torch.save(train_labels, path + 'train_labels.pt')\n",
        "torch.save(val_labels, path + 'val_labels.pt')\n",
        "torch.save(test_labels, path + 'test_labels.pt')"
      ],
      "metadata": {
        "id": "tHkmyzMC4mnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the features from my drive\n",
        "\n",
        "path = F\"/content/gdrive/My Drive/lab_2_features/\" \n",
        "\n",
        "train_features = torch.load(path + 'train_features.pt')\n",
        "val_features = torch.load(path + 'val_features.pt')\n",
        "test_features = torch.load(path + 'test_features.pt')\n",
        "\n",
        "train_labels = torch.load(path + 'train_labels.pt')\n",
        "val_labels = torch.load(path + 'val_labels.pt')\n",
        "test_labels = torch.load(path + 'test_labels.pt')"
      ],
      "metadata": {
        "id": "3OUdsR6K4uUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#because the AlexNet feature computation split the features and labels apart, I'll need to combine features and label data into one matrix back again\n",
        "train_featlabels = []\n",
        "val_featlabels = []\n",
        "test_featlabels = []\n",
        "\n",
        "for i in range(len(train_features)):\n",
        "  train_featlabels.append([train_features[i], train_labels[i].type(torch.LongTensor)])\n",
        "\n",
        "for i in range(len(val_features)):\n",
        "  val_featlabels.append([val_features[i], val_labels[i].type(torch.LongTensor)])\n",
        "\n",
        "for i in range(len(test_features)):\n",
        "  test_featlabels.append([test_features[i], test_labels[i].type(torch.LongTensor)])  \n"
      ],
      "metadata": {
        "id": "i_frVhgO4271"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNetClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(AlexNetClassifier, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(256, 128, 2)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.fc1 = nn.Linear(128*2*2, 32)\n",
        "    self.fc2 = nn.Linear(32,9)\n",
        "    \n",
        "  def forward(self, features):\n",
        "    x = self.pool(F.relu((self.conv1(features))))\n",
        "    x = x.view(-1, 128*2*2)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    x = x.squeeze(1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "70bzxS7q5SV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to train this AlexNet model..."
      ],
      "metadata": {
        "id": "BD-aRKJ45ca5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = True\n",
        "\n",
        "model = AlexNetClassifier()\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "\n",
        "train_loss, train_err, val_loss, val_err = train(model, train_featlabels, val_featlabels, learning_rate = 0.0025)\n",
        "\n",
        "plot_training_curve(train_loss, train_err, val_loss, val_err)\n",
        "\n",
        "print(\"Lowest Validation Error: {}, at Epoch {}\".format(min(val_err), np.argmin(val_err) + 1))\n",
        "print(\"Lowest Validation Loss: {}, at Epoch {}\".format(min(val_loss), np.argmin(val_err) + 1))"
      ],
      "metadata": {
        "id": "LNDyt80q5bV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the test accuracy on the test data..."
      ],
      "metadata": {
        "id": "t20ee4pL5o5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "eval_total = 0\n",
        "eval_incorrect = 0\n",
        "\n",
        "eval_accu_list = []\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(dataset=test_featlabels, shuffle = True, batch_size=128)\n",
        "\n",
        "for data in tqdm(testloader):\n",
        "    inputs,labels=data\n",
        "\n",
        "    #############################################\n",
        "    #To Enable GPU Usage\n",
        "    if use_cuda and torch.cuda.is_available():\n",
        "      inputs = inputs.cuda()\n",
        "      labels = labels.cuda()\n",
        "    #############################################\n",
        "\n",
        "    outputs=model(inputs)\n",
        "\n",
        "    loss= criterion(outputs,labels)\n",
        "\n",
        "    _, predicted = outputs.max(1)\n",
        "    eval_total += labels.size(0)\n",
        "    eval_incorrect += predicted.ne(labels).sum().item()\n",
        "\n",
        "eval_accu=100.*eval_incorrect/eval_total\n",
        "\n",
        "eval_accu_list.append(eval_accu)\n",
        "\n",
        "print('Test Error: %.3f'%(eval_accu))\n",
        "\n",
        "print('Test Accuracy: ', 100-eval_accu)"
      ],
      "metadata": {
        "id": "gvcoW2eg5mp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part 3: Testing on New Data"
      ],
      "metadata": {
        "id": "U2bflo3B5yI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I take my own American Sign Language hand gesture photos and test them on my model. This set contains 27 images (3 photos of alphabets A to I)"
      ],
      "metadata": {
        "id": "ylW0ci7C50yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define transform \n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize([224, 224]), #ensure all photos will be 224x224\n",
        "    transforms.CenterCrop(224), #assume photos have hand centered\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]) #normalize with mean and std values of 0.5\n",
        "\n",
        "my_dataset = torchvision.datasets.ImageFolder('/content/gdrive/MyDrive/My_Hand_Gestures',\n",
        "                                           transform=transform)"
      ],
      "metadata": {
        "id": "JH18cXou5-EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(my_dataset, batch_size=128)\n",
        "\n",
        "test_features = torch.FloatTensor([])\n",
        "test_labels = torch.FloatTensor([])\n",
        "for images, labels in iter(test_loader):\n",
        "  test_features = torch.cat((test_features, alexNet.features(images)), 0)\n",
        "  test_labels = torch.cat((test_labels, labels), 0)\n",
        "\n",
        "test_featlabels = []\n",
        "for i in range(len(test_features)):\n",
        "  test_featlabels.append([test_features[i], test_labels[i].type(torch.LongTensor)])"
      ],
      "metadata": {
        "id": "NqqbG1Ek6B_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing test accuracy..."
      ],
      "metadata": {
        "id": "Y7wtNqxI6FLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model #use last model, which had the best error at the last epoch\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "eval_total = 0\n",
        "eval_incorrect = 0\n",
        "\n",
        "eval_accu_list = []\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(dataset=test_featlabels, shuffle = True, batch_size=128)\n",
        "\n",
        "for data in tqdm(testloader):\n",
        "    inputs,labels=data\n",
        "\n",
        "    #############################################\n",
        "    #To Enable GPU Usage\n",
        "    if use_cuda and torch.cuda.is_available():\n",
        "      inputs = inputs.cuda()\n",
        "      labels = labels.cuda()\n",
        "    #############################################\n",
        "\n",
        "    outputs=model(inputs)\n",
        "\n",
        "    loss= criterion(outputs,labels)\n",
        "\n",
        "    _, predicted = outputs.max(1)\n",
        "    eval_total += labels.size(0)\n",
        "    eval_incorrect += predicted.ne(labels).sum().item()\n",
        "\n",
        "eval_accu=100.*eval_incorrect/eval_total\n",
        "\n",
        "eval_accu_list.append(eval_accu)\n",
        "\n",
        "print('Test Error: %.3f'%(eval_accu))\n",
        "\n",
        "print('Test Accuracy: ', 100-eval_accu)"
      ],
      "metadata": {
        "id": "DjMJBjR16HSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that my model is able to accuracy predict all my 27 hand gestures!"
      ],
      "metadata": {
        "id": "N1d9ApNy6JNR"
      }
    }
  ]
}